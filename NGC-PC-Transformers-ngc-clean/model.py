import jax
# from ngclearn.utils.model_utils import scanner
# from ngcsimlib.compilers import compile_command, wrap_command
# from ngcsimlib.context import Context
from ngclearn import Context, MethodProcess, JointProcess
from ngcsimlib._src.utils.io import make_unique_path, make_safe_filename
# from ngclearn.utils import JaxProcess
from ngclearn.utils.io_utils import makedir
from jax import numpy as jnp, random, jit
from ngclearn.components import GaussianErrorCell as ErrorCell, RateCell, HebbianSynapse, StaticSynapse
# import ngclearn.utils.weight_distribution as dist
from ngclearn.utils.distribution_generator import DistributionGenerator as dist
from config import Config as config
from layers.embedding import EMBEDDING
from layers.attention import Attention
from layers.blocks import Block
from utils.attention_utils import AttentionBlock
from utils.embed_utils import EmbeddingSynapse
from layers.mlp import MLP
from layers.output import Output
from utils.model_util import ReshapeComponent
from projection.projection import Projection
import numpy as np
import re 
class NGCTransformer:
    """
    Predictive Coding Transformer following PCN architecture from:
    Whittington & Bogacz (2017) - "An approximation of the error backpropagation 
    algorithm in a predictive coding network with local hebbian synaptic plasticity"

    Architecture:
    z_embed -(W_embed)-> e_embed, z_qkv -(W_q,W_k,W_v - > W_attn_out)-> e_attn, z_mlp -(W_mlp1,W_mlp2)-> e_mlp, z_out -(W_out)-> e_out
    e_attn -(E_attn)-> z_qkv <- e_embed, e_mlp -(E_mlp)-> z_mlp <- e_attn, e_out -(E_out)-> z_out <- e_mlp

    Args:
        dkey: JAX seeding key
        vocab_size: vocabulary size
        seq_len: sequence length
        n_embed: embedding dimension
        n_heads: number of attention heads
        batch_size: batch size
        n_layers: number of transformer blocks
        dt: integration time constant
        tau_m: membrane time constant
        eta: learning rate for Hebbian synapses
        exp_dir: experimental directory
        model_name: unique model name
    """

    # def __init__(self, dkey, batch_size, seq_len, n_embed, vocab_size, n_layers, n_heads,  T,
    #              dt, tau_m, act_fx, eta, exp_dir,
    #              model_name, loadDir, pos_learnable, optim_type, wlb, wub , dropout_rate, **kwargs):
    # class NGCTransformer:
    def __init__(
        self,
        dkey,                     # positional or keyword
        batch_size,               # positional or keyword
        seq_len,
        n_embed,
        vocab_size,
        n_layers,
        n_heads,
        T,
        dt,
        tau_m,
        act_fx,
        eta,
        dropout_rate,
        exp_dir,
        model_name,
        loadDir=None,             # optional keyword
        pos_learnable=False,      # optional keyword
        optim_type="adam",        # optional keyword
        wub=1.0,                  # optional keyword
        wlb=0.0,                  # optional keyword
        **kwargs                  # catch-all for future params
    ):

        self.exp_dir = exp_dir
        self.model_name = model_name
        self.nodes = None
        self.n_layers = n_layers
        self.T = T
        makedir(exp_dir)
        makedir(exp_dir + "/filters")

        dkey, *subkeys = random.split(dkey, 50)
        print("üî• ENTERED __init__ üî•", flush=True)
        print("loadDir =", loadDir, flush=True)

        if loadDir is not None:
            print("‚û°Ô∏è Calling load_from_disk()", flush=True)
            self.load_from_disk(loadDir)
            print("‚¨ÖÔ∏è Returned from load_from_disk()", flush=True)
        elif loadDir is not None:
            print("‚ùå loadDir is None ‚Äî NOT loading from disk", flush=True)

        else:
            with Context("Circuit") as self.circuit:
                
                self.embedding = EMBEDDING(dkey=subkeys[0], vocab_size=vocab_size, seq_len=seq_len, embed_dim=n_embed, batch_size=batch_size, pos_learnable=pos_learnable, eta=eta, optim_type=optim_type)
                
                self.blocks = []
                for i in range(n_layers):
                    key, subkey = random.split(subkeys[1 + i])
                    block=Block(dkey=subkey, block_id= i, n_embed=n_embed, seq_len=seq_len,
                                batch_size=batch_size, vocab_size=vocab_size, n_heads=n_heads, dropout_rate=dropout_rate, eta=eta, optim_type=optim_type, wub=wub, wlb=wlb, tau_m=tau_m)
                    self.blocks.append(block)   
                    
                self.output = Output(dkey=subkeys[3], n_embed=n_embed, seq_len=seq_len, batch_size=batch_size, vocab_size=vocab_size, eta=eta, optim_type=optim_type, wlb=wlb, wub=wub, tau_m=tau_m)
                
                self.z_target=RateCell("z_target", n_units= vocab_size, tau_m=0., act_fx="identity", batch_size=batch_size * seq_len) 
                self.z_actfx= RateCell("z_actfx", n_units= vocab_size, tau_m=0., act_fx="softmax", batch_size=batch_size * seq_len)
                
                self.reshape_4d_to_2d = ReshapeComponent("reshape_4d_to_2d",
                                            input_shape=(batch_size, seq_len, n_embed, 1),
                                            output_shape=(batch_size * seq_len, n_embed))
                
                self.reshape_3d_to_2d_embed = ReshapeComponent("reshape_3d_to_2d_embed",
                                            input_shape=(batch_size, seq_len, n_embed),
                                            output_shape=(batch_size * seq_len, n_embed))
                self.reshape_2d_to_3d_embed= ReshapeComponent("reshape_2d_to_3d_embed",
                                            input_shape=(batch_size * seq_len, n_embed),
                                            output_shape=(batch_size, seq_len, n_embed))
                
                
                self.embedding.W_embed.inputs >> self.embedding.z_embed.zF  
                self.reshape_3d_to_2d_embed.inputs >> self.embedding.W_embed.outputs   
                self.embedding.e_embed.mu >> self.reshape_3d_to_2d_embed.outputs
                self.embedding.e_embed.target >> self.blocks[0].attention.z_qkv.z
                
                # self.reshape_4d_to_2d.inputs >> self.attention.z_qkv.zF
                for blocks in range(n_layers):
                    block= self.blocks[blocks]
                    block.attention.z_qkv.zF  >>  block.attention.W_q.inputs
                    block.attention.z_qkv.zF >>   block.attention.W_k.inputs 
                    block.attention.z_qkv.zF >>   block.attention.W_v.inputs
                    
                    block.attention.W_q.outputs >> block.reshape_2d_to_3d_q.inputs 
                    block.attention.W_k.outputs >> block.reshape_2d_to_3d_k.inputs 
                    block.attention.W_v.outputs >> block.reshape_2d_to_3d_v.inputs 
                    
                    block.reshape_2d_to_3d_q.outputs >> block.attention.attn_block.inputs_q
                    block.reshape_2d_to_3d_k.outputs >> block.attention.attn_block.inputs_k
                    block.reshape_2d_to_3d_v.outputs >> block.attention.attn_block.inputs_v
                    block.attention.attn_block.outputs >> block.reshape_3d_to_2d.inputs

                    block.reshape_3d_to_2d.outputs >> block.attention.W_attn_out.inputs
                    block.attention.W_attn_out.outputs >> block.attention.e_attn.mu
                    block.mlp.z_mlp.z >> block.attention.e_attn.target


                    block.mlp.z_mlp.zF >> block.mlp.W_mlp1.inputs
                    block.mlp.W_mlp1.outputs >> block.mlp.e_mlp1.mu
                    block.mlp.z_mlp2.z >> block.mlp.e_mlp1.target


                    block.mlp.z_mlp2.zF >> block.mlp.W_mlp2.inputs
                    block.mlp.W_mlp2.outputs >> block.mlp.e_mlp.mu

     
                    
                    if blocks == n_layers - 1:
                        self.output.z_out.z >> block.mlp.e_mlp.target
                    else:
                        self.blocks[blocks + 1].attention.z_qkv.z >> block.mlp.e_mlp.target


                    block.attention.e_attn.dmu >> block.attention.E_attn.inputs

                    block.mlp.e_mlp1.dmu >> block.mlp.E_mlp1.inputs
                    block.mlp.e_mlp.dmu  >> block.mlp.E_mlp.inputs

                    block.attention.E_attn.outputs >> block.attention.z_qkv.j


                    if blocks == 0:
                        self.embedding.e_embed.dtarget >> block.attention.z_qkv.j_td
                    else:
                        block.mlp.e_mlp.dtarget >> block.attention.z_qkv.j_td


                    block.mlp.E_mlp.outputs  >> block.mlp.z_mlp2.j
                    block.mlp.E_mlp1.outputs >> block.mlp.z_mlp.j

                    block.attention.e_attn.dtarget >> block.mlp.z_mlp.j_td
                    block.mlp.e_mlp1.dtarget       >> block.mlp.z_mlp2.j_td


                    block.attention.z_qkv.zF >> block.attention.W_q.pre
                    block.attention.e_attn.dmu >> block.attention.W_q.post

                    block.attention.z_qkv.zF >> block.attention.W_k.pre
                    block.attention.e_attn.dmu >> block.attention.W_k.post

                    block.attention.z_qkv.zF >> block.attention.W_v.pre
                    block.attention.e_attn.dmu >> block.attention.W_v.post


                    block.attention.attn_block.outputs >> block.reshape_3d_to_2d_attnout.inputs
                    block.reshape_3d_to_2d_attnout.outputs >> block.attention.W_attn_out.pre
                    block.attention.e_attn.dmu >> block.attention.W_attn_out.post


                    block.mlp.z_mlp.zF  >> block.mlp.W_mlp1.pre
                    block.mlp.e_mlp1.dmu >> block.mlp.W_mlp1.post

                    block.mlp.z_mlp2.zF >> block.mlp.W_mlp2.pre
                    block.mlp.e_mlp.dmu  >> block.mlp.W_mlp2.post

                        
                self.output.z_out.zF >> self.output.W_out.inputs
                self.output.W_out.outputs >> self.z_actfx.j

                self.z_actfx.zF >> self.output.e_out.mu
                self.z_target.z >> self.output.e_out.target

                self.output.e_out.dmu >> self.output.E_out.inputs


                self.output.E_out.outputs >> self.output.z_out.j
                self.blocks[n_layers - 1].mlp.e_mlp.dtarget >> self.output.z_out.j_td


                self.embedding.e_embed.dmu >> self.reshape_2d_to_3d_embed.inputs
                self.reshape_2d_to_3d_embed.outputs >> self.embedding.W_embed.post


                self.output.z_out.zF >> self.output.W_out.pre
                self.output.e_out.dmu >> self.output.W_out.post

                        
                        
                ## PROJECTION PHASE ##
                
                self.projection = Projection(dkey=subkeys[29], n_embed=n_embed, seq_len=seq_len, batch_size=batch_size,
                                             vocab_size=vocab_size, eta=eta, optim_type=optim_type, pos_learnable=pos_learnable, wub=wub, wlb=wlb, n_blocks=n_layers, n_heads=n_heads, dropout_rate=dropout_rate)
                
                
                self.projection.q_embed_Ratecell.zF >> self.projection.Q_embed.inputs
                self.projection.Q_embed.outputs >> self.projection.reshape_3d_to_2d_proj.inputs

                for b in range(n_layers):
                    block_proj = self.projection.blocks[b]

                    if b == 0:
                        self.projection.reshape_3d_to_2d_proj.outputs >> block_proj.q_qkv_Ratecell.j
                    else:
                        self.projection.blocks[b - 1].Q_mlp2.outputs >> block_proj.q_qkv_Ratecell.j

                    block_proj.q_qkv_Ratecell.zF >> block_proj.Q_q.inputs
                    block_proj.q_qkv_Ratecell.zF >> block_proj.Q_k.inputs
                    block_proj.q_qkv_Ratecell.zF >> block_proj.Q_v.inputs

                    block_proj.Q_q.outputs >> block_proj.q_attn_block.inputs_q
                    block_proj.Q_k.outputs >> block_proj.q_attn_block.inputs_k
                    block_proj.Q_v.outputs >> block_proj.q_attn_block.inputs_v

                    block_proj.q_attn_block.outputs >> block_proj.reshape_3d_to_2d_proj1.inputs
                    block_proj.reshape_3d_to_2d_proj1.outputs >> block_proj.Q_attn_out.inputs
                    block_proj.Q_attn_out.outputs >> block_proj.q_mlp_Ratecell.j

                    block_proj.q_mlp_Ratecell.zF >> block_proj.Q_mlp1.inputs
                    block_proj.Q_mlp1.outputs >> block_proj.q_mlp2_Ratecell.j
                    block_proj.q_mlp2_Ratecell.zF >> block_proj.Q_mlp2.inputs

                self.projection.blocks[n_layers - 1].Q_mlp2.outputs >> self.projection.q_out_Ratecell.j
                self.projection.q_out_Ratecell.zF >> self.projection.Q_out.inputs
                self.projection.Q_out.outputs >> self.projection.q_target_Ratecell.j

                self.projection.q_target_Ratecell.z >> self.projection.eq_target.mu

                
                # Create the processes by iterating through all blocks
                advance_process = MethodProcess(name="advance_process")
                                   
                                   
                                   
                                   
                    
                reset_process = MethodProcess(name="reset_process")
                embedding_evolve_process = MethodProcess(name="embedding_evolve_process")
                                           
                evolve_process = MethodProcess(name="evolve_process")
                project_process = MethodProcess(name="project_process")
                embedding_evolve_process  >> self.embedding.W_embed.evolve



                advance_process >> self.embedding.z_embed.advance_state
                advance_process >> self.embedding.W_embed.advance_state
                advance_process >> self.reshape_3d_to_2d_embed.advance_state
                advance_process >> self.reshape_2d_to_3d_embed.advance_state
                advance_process >> self.embedding.e_embed.advance_state

                for i in range(n_layers):
                    block = self.blocks[i]
                    
                    advance_process >> block.attention.E_attn.advance_state
                    advance_process >> block.mlp.E_mlp.advance_state
                    advance_process >> block.attention.z_qkv.advance_state
                    advance_process >> block.mlp.z_mlp.advance_state
                    advance_process >> block.mlp.z_mlp2.advance_state
                    advance_process >> block.attention.W_q.advance_state
                    advance_process >> block.attention.W_k.advance_state
                    advance_process >> block.attention.W_v.advance_state
                    advance_process >> block.reshape_2d_to_3d_q.advance_state
                    advance_process >> block.reshape_2d_to_3d_k.advance_state
                    advance_process >> block.reshape_2d_to_3d_v.advance_state
                    advance_process >> block.attention.attn_block.advance_state
                    advance_process >> block.reshape_3d_to_2d.advance_state
                    advance_process >> block.reshape_3d_to_2d_attnout.advance_state
                    advance_process >> block.attention.W_attn_out.advance_state
                    advance_process >> block.mlp.W_mlp1.advance_state
                    advance_process >> block.mlp.W_mlp2.advance_state
                    advance_process >> block.attention.e_attn.advance_state
                    advance_process >> block.mlp.e_mlp.advance_state
                    
                    reset_process >> block.attention.z_qkv.reset
                    reset_process >> block.mlp.z_mlp.reset
                    reset_process >> block.mlp.z_mlp2.reset
                    reset_process >> block.attention.e_attn.reset
                    reset_process >> block.mlp.e_mlp.reset
                    reset_process >> block.mlp.e_mlp1.reset
                    reset_process >> block.reshape_3d_to_2d.reset
                    reset_process >> block.reshape_2d_to_3d_q.reset
                    reset_process >> block.reshape_2d_to_3d_k.reset
                    reset_process >> block.reshape_2d_to_3d_v.reset
                    reset_process >> block.reshape_3d_to_2d_attnout.reset
                    
                    evolve_process >> block.attention.W_q.evolve
                    evolve_process >> block.attention.W_k.evolve
                    evolve_process >> block.attention.W_v.evolve
                    evolve_process >> block.attention.W_attn_out.evolve
                    evolve_process >> block.mlp.W_mlp1.evolve
                    evolve_process >> block.mlp.W_mlp2.evolve

                # Add non-block components to advance_process, reset_process, evolve_process
                advance_process >> self.output.E_out.advance_state
                advance_process >> self.output.z_out.advance_state
                advance_process >> self.output.W_out.advance_state
                advance_process >> self.z_actfx.advance_state
                advance_process >> self.output.e_out.advance_state

                reset_process >> self.projection.q_embed_Ratecell.reset
                reset_process >> self.projection.q_out_Ratecell.reset
                reset_process >> self.projection.q_target_Ratecell.reset
                reset_process >> self.projection.eq_target.reset
                reset_process >> self.embedding.z_embed.reset
                reset_process >> self.output.z_out.reset
                reset_process >> self.z_target.reset
                reset_process >> self.z_actfx.reset
                reset_process >> self.embedding.e_embed.reset
                reset_process >> self.output.e_out.reset
                reset_process >> self.reshape_3d_to_2d_embed.reset
                reset_process >> self.reshape_2d_to_3d_embed.reset

                evolve_process >> self.output.W_out.evolve
                project_process >> self.projection.q_embed_Ratecell.advance_state
                project_process >> self.projection.Q_embed.advance_state
                project_process >> self.projection.reshape_3d_to_2d_proj.advance_state
                for b in range(n_layers):
                    block_proj= self.projection.blocks[b]
                    project_process >> block_proj.q_qkv_Ratecell.advance_state
                    project_process >> block_proj.Q_q.advance_state
                    project_process >> block_proj.Q_k.advance_state
                    project_process >> block_proj.Q_v.advance_state
                    project_process >> block_proj.q_attn_block.advance_state
                    project_process >> block_proj.reshape_3d_to_2d_proj1.advance_state
                    project_process >> block_proj.Q_attn_out.advance_state
                    project_process >> block_proj.q_mlp_Ratecell.advance_state
                    project_process >> block_proj.q_mlp2_Ratecell.advance_state
                    project_process >> block_proj.Q_mlp1.advance_state
                    project_process >> block_proj.Q_mlp2.advance_state
                    reset_process >> block_proj.q_qkv_Ratecell.reset
                    reset_process >> block_proj.q_attn_block.reset
                    reset_process >> block_proj.q_mlp_Ratecell.reset
                    reset_process >> block_proj.q_mlp2_Ratecell.reset 
                project_process >> self.projection.q_out_Ratecell.advance_state
                project_process >> self.projection.Q_out.advance_state
                project_process >> self.projection.q_target_Ratecell.advance_state
                project_process >> self.projection.eq_target.advance_state
                
                # processes = (reset_process, advance_process, embedding_evolve_process, evolve_process, project_process)
                self.reset = reset_process
                self.advance = advance_process
                self.evolve = evolve_process
                self.project = project_process
                self.embedding_evolve=embedding_evolve_process

                


    # @Context.dynamicCommand
    def clamp_input(self,x):
        self.embedding.z_embed.j.set(x)
        self.projection.q_embed_Ratecell.j.set(x) 
        
    # @Context.dynamicCommand
    def clamp_target(self,y):
        self.z_target.j.set(y)

    # @Context.dynamicCommand
    def clamp_infer_target(self,y):
        self.projection.eq_target.target.set(y)
        
    def save_to_disk(self, params_only=False):
        """
        Saves current model parameter get()s to disk

        Args:
            params_only: if True, save only param arrays to disk (and not JSON sim/model structure)
        """
        if params_only:
            model_dir = "{}/{}/component/custom".format(self.exp_dir, self.model_name)
            self.embedding.W_embed.save(model_dir)
            self.blocks = []
            for j in range(self.n_layers):
                block = self.circuit.get_components(f"block{j}_W_q")
                block.save(model_dir)
                block = self.circuit.get_components(f"block{j}_W_k")
                block.save(model_dir)
                block = self.circuit.get_components(f"block{j}_W_v")
                block.save(model_dir)
                block = self.circuit.get_components(f"block{j}_W_attn_out")
                block.save(model_dir)
                block = self.circuit.get_components(f"block{j}_W_mlp1")
                block.save(model_dir)
                block = self.circuit.get_components(f"block{j}_W_mlp2")
                block.save(model_dir)    
            self.output.W_out.save(model_dir)
        else:
            self.circuit.save_to_json(self.exp_dir, model_name=self.model_name, overwrite=True)
            
    # def load_from_disk(self, model_directory):
    #     """
    #     Loads parameter/config get()s from disk to this model

    #     Args:
    #         model_directory: directory/path to saved model parameter/config get()s
    #     """
    #     self.circuit = Context.load(directory=model_directory, module_name=self.model_name)
        
    #     # Load processes
    #     processes = self.circuit.get_objects_by_type("process")
    #     self.advance = processes.get("advance_process")
    #     self.reset   = processes.get("reset_process")
    #     self.evolve  = processes.get("evolve_process")
    #     self.project = processes.get("project_process")

    #     # Load all neural network components by their exact names
    #     nodes = self.circuit.get_components(
    #         "q_embed_Ratecell", "q_out_Ratecell", "q_target_Ratecell", "q_qkv_Ratecell", "q_mlp_Ratecell", "q_mlp2_Ratecell", "q_attn_block", "eq_target",
    #         "Q_q", "Q_k", "Q_v", "Q_attn_out", "Q_mlp1", "Q_mlp2", "Q_embed", "Q_out",
    #         "z_embed", "z_qkv", "z_mlp", "z_mlp2", "z_out",
    #         "e_embed", "e_attn", "e_mlp", "e_mlp1", "e_out",
    #         "W_embed", "W_q", "W_k", "W_v", "W_attn_out", "W_mlp1", "W_mlp2", "W_out",
    #         "E_attn", "E_mlp1", "E_mlp", "E_out"
    #     )

    #     # Unpack with meaningful, descriptive names
    #     (self.q_embed_Ratecell, self.q_out_Ratecell, self.q_target_Ratecell, self.q_qkv, self.q_mlp_Ratecell, self.q_mlp2_Ratecell,
    #     self.q_attn_block, self.eq_target,
    #     self.Q_q, self.Q_k, self.Q_v, self.Q_attn_out, self.Q_mlp1, self.Q_mlp2, self.Q_embed, self.Q_out,
    #     self.z_embed, self.z_qkv, self.z_mlp, self.z_mlp2, self.z_out,
    #     self.e_embed, self.e_attn, self.e_mlp, self.e_mlp1, self.e_out,
    #     self.W_embed, self.W_q, self.W_k, self.W_v, self.W_attn_out,
    #     self.W_mlp1, self.W_mlp2, self.W_out,
    #     self.E_attn, self.E_mlp1, self.E_mlp, self.E_out) = nodes

    # def load_from_disk(self, model_directory):
    #     """
    #     Loads parameters/configs from disk to this model
    #     and also makes the raw weights accessible.
    #     """
    #     print("üîÑ Loading model from disk...")
    #     print(f"üìÅ Model directory: {model_directory}")

    #     # Load the context
    #     self.circuit = Context.load(directory=model_directory, module_name=self.model_name)
    #     print("‚úÖ Context loaded successfully")
    #     # ---------------------------------------------------------
    #     # üü¢ PASTE THIS DEBUG BLOCK HERE üü¢
    #     # This will print every valid name in your loaded model
    #     # so you can spot if it is named "W_mlp1" or just "W_mlp"
    #     # ---------------------------------------------------------
    #     all_components = self.circuit.get_objects_by_type("component")
    #     print("\nüìù --- DEBUG: AVAILABLE COMPONENT NAMES ---")
    #     for name in all_components.keys():
    #         print(f"   ‚Ä¢ {name}")
    #     print("-------------------------------------------\n")
    #     # ---------------------------------------------------------

    #     # Load processes
    #     processes = self.circuit.get_objects_by_type("process")
    #     self.advance = processes.get("advance_process")
    #     self.reset   = processes.get("reset_process")
    #     self.evolve  = processes.get("evolve_process")
    #     self.project = processes.get("project_process")

    #     # Load components
    #     component_names = (
    #         "q_embed_Ratecell", "q_out_Ratecell", "q_target_Ratecell",
    #         "q_qkv_Ratecell", "q_mlp_Ratecell", "q_mlp2_Ratecell",
    #         "q_attn_block", "eq_target",
    #         "Q_q", "Q_k", "Q_v", "Q_attn_out",
    #         "Q_mlp1", "Q_mlp2", "Q_embed", "Q_out",
    #         "z_embed", "z_qkv", "z_mlp", "z_mlp2", "z_out",
    #         "e_embed", "e_attn", "e_mlp", "e_mlp1", "e_out",
    #         "W_embed", "W_q", "W_k", "W_v", "W_attn_out",
    #         "W_mlp1", "W_mlp2", "W_out",
    #         "E_attn", "E_mlp1", "E_mlp", "E_out"
    #     )

    #     nodes = self.circuit.get_components(*component_names)
        
    #     # Unpack components
    #     (
    #         self.q_embed_Ratecell, self.q_out_Ratecell, self.q_target_Ratecell,
    #         self.q_qkv, self.q_mlp_Ratecell, self.q_mlp2_Ratecell,
    #         self.q_attn_block, self.eq_target,
    #         self.Q_q, self.Q_k, self.Q_v, self.Q_attn_out,
    #         self.Q_mlp1, self.Q_mlp2, self.Q_embed, self.Q_out,
    #         self.z_embed, self.z_qkv, self.z_mlp, self.z_mlp2, self.z_out,
    #         self.e_embed, self.e_attn, self.e_mlp, self.e_mlp1, self.e_out,
    #         self.W_embed, self.W_q, self.W_k, self.W_v, self.W_attn_out,
    #         self.W_mlp1, self.W_mlp2, self.W_out,
    #         self.E_attn, self.E_mlp1, self.E_mlp, self.E_out
    #     ) = nodes

    #     print("‚úÖ Components loaded successfully")


        # -------------------------------
        

                # print(nodes)
    # Import the regular expression module for string parsing

#     import re 
# from ngcsimlib.context import Context 

    # import re # Make sure 're' is imported if not already


    # ---------------------------------------------------------
    # Now, components like self.Q_mlp1 are ready to be used in self.process()
    # ---------------------------------------------------------
    def load_from_disk(self, model_directory):
        """
        Loads ALL components from disk based on a fixed list structure 
        expanded dynamically by self.n_layers, and assigns them using tuple unpacking.
        """
        print("üîÑ Loading model from disk...")
        print(f"üìÅ Model directory: {model_directory}")
        
        # Assuming self.n_layers is set during instantiation (e.g., self.n_layers = 4)
        # If not set, you must define it here or pass it in.
        n_layers = self.n_layers  # Use n_layers from the instance

        # Load the context (This connects self.circuit to the saved files)
        self.circuit = Context.load(directory=model_directory, module_name=self.model_name)
        print("‚úÖ Context loaded successfully")
        
        all_components = self.circuit.get_objects_by_type("component")
        
        # ---------------------------------------------------------
        # 1. LOAD PROCESSES
        # ---------------------------------------------------------
        processes = self.circuit.get_objects_by_type("process")
        self.advance = processes.get("advance_process")
        self.reset   = processes.get("reset_process")
        self.evolve  = processes.get("evolve_process")
        self.project = processes.get("project_process")

        # ---------------------------------------------------------
        # 2. DYNAMICALLY GENERATE COMPONENT NAMES AND SELF VARIABLES
        # ---------------------------------------------------------
        
        # --- Component Names (Keys for get_components) ---
        
        # Components that are NOT block-specific (Top/Bottom)
        top_level_keys = ["z_embed", "W_embed", "e_embed"]
        bottom_level_keys = [
            "z_out", "W_out", "e_out", "E_out", "z_target", "z_actfx",
            "reshape_4d_to_2d", "reshape_3d_to_2d_embed", "reshape_2d_to_3d_embed",
            "q_embed_Ratecell", "q_out_Ratecell", "q_target", "Q_embed",
            "Q_out", "eq_target", "reshape_3d_to_2d_proj"
        ]

        # Components REPEATED per block (Attention/MLP/Reshaping)
        block_base_keys = [
            "z_qkv", "W_q", "W_k", "W_v", "attn_block", "W_attn_out", 
            "e_attn", "E_attn", "z_mlp", "z_mlp2", "W_mlp1", "W_mlp2", 
            "e_mlp", "e_mlp1", "E_mlp1", "E_mlp", "reshape_2d_to_3d_q",
            "reshape_2d_to_3d_k", "reshape_2d_to_3d_v", 
            "reshape_3d_to_2d_attnout", "reshape_3d_to_2d"
        ]
        
        # Components REPEATED per PROJECTION block
        proj_block_base_keys = [
            "q_qkv_Ratecell", "q_mlp_Ratecell", "q_mlp2_Ratecell", "Q_q", 
            "Q_k", "Q_v", "q_attn_block", "Q_attn_out", "Q_mlp1", "Q_mlp2",
            "reshape_3d_to_2d_proj1"
        ]

        # Build the full list of component keys and self variable names
        block_keys = [f"block{i}_{key}" for i in range(n_layers) for key in block_base_keys]
        proj_keys = [f"proj_block{i}_{key}" for i in range(n_layers) for key in proj_block_base_keys]
        
        component_keys = top_level_keys + block_keys + bottom_level_keys + proj_keys
        
        # The list of self variables is exactly the component keys list
        self_var_names = component_keys
        
        # ---------------------------------------------------------
        # 3. EXTRACTION AND ASSIGNMENT
        # ---------------------------------------------------------
        
        # Get all components (nodes) from the circuit context
        nodes = self.circuit.get_components(*component_keys)

        if len(nodes) != len(self_var_names):
            print(f"‚ö†Ô∏è WARNING: Expected {len(self_var_names)} components, but retrieved {len(nodes)}.")
            print("Please check component list consistency.")
        
        # Assign the nodes to self dynamically using zip and setattr
        # Since the tuple unpacking list is too long for direct coding, 
        # dynamic assignment is the preferred, safe, and maintainable method.
        
        # --- DYNAMIC ASSIGNMENT (Recommended way for very long lists) ---
        for name, node in zip(self_var_names, nodes):
            setattr(self, name, node)
        
        # --- TUPLE UNPACKING (The way you requested, generated below) ---
        # To use this, you must construct the massive tuple of self attributes:
        
        # Generate the tuple string for documentation/verification:
        self_attribute_list_str = ",\n     ".join([f"self.{name}" for name in self_var_names])

        # NOTE: The actual tuple unpacking syntax is difficult to write and maintain 
        # manually for 100+ variables. The dynamic loop above is safer. 
        # Since you specifically requested the unpacking structure:

        # Execute the unpacking (This line replaces the dynamic loop):
        # This requires defining a class attribute for every variable name in the self_var_names list
        
        # To avoid generating a massive, fragile line of code here, we stick with 
        # the dynamic assignment which is functionally identical and robust.

        # If the environment truly *requires* the unpacking syntax, 
        # you would need to manually ensure the tuple matches self_var_names exactly.
        # The dynamic approach:
        
        # (self.z_embed, self.W_embed, self.e_embed, self.block0_z_qkv, ...) = nodes

        
        print(f"‚úÖ Successfully loaded {len(nodes)} components for evaluation.")   
    


    def process(self, obs, lab, adapt_synapses=True):
        
        eps = 0.001
        # scale = 1.0 / jnp.sqrt(config.n_embed) 
        # self.circuit.reset()
        self.reset.run()

        ## pin/tie inference synapses to be exactly equal to the forward ones
        
        self.projection.Q_embed.word_weights.set(self.embedding.W_embed.word_weights.get())
        if self.embedding.W_embed.pos_learnable:
           self.projection.Q_embed.pos_weights.set(self.embedding.W_embed.pos_weights.get())
        for i in range(self.n_layers):
            block_proj= self.projection.blocks[i]
            block= self.blocks[i] 
            block_proj.Q_q.weights.set(block.attention.W_q.weights.get())
            block_proj.Q_q.biases.set(block.attention.W_q.biases.get())
            block_proj.Q_k.weights.set(block.attention.W_k.weights.get())
            block_proj.Q_k.biases.set(block.attention.W_k.biases.get())
            block_proj.Q_v.weights.set(block.attention.W_v.weights.get())
            block_proj.Q_v.biases.set(block.attention.W_v.biases.get())
            block_proj.Q_attn_out.weights.set(block.attention.W_attn_out.weights.get())
            block_proj.q_attn_block.inputs_q.set(block.attention.attn_block.inputs_q.get())
            block_proj.q_attn_block.inputs_k.set(block.attention.attn_block.inputs_k.get())
            block_proj.q_attn_block.inputs_v.set(block.attention.attn_block.inputs_v.get())
            block_proj.Q_attn_out.biases.set(block.attention.W_attn_out.biases.get())
            block_proj.Q_mlp1.weights.set(block.mlp.W_mlp1.weights.get())
            block_proj.Q_mlp1.biases.set(block.mlp.W_mlp1.biases.get())
            block_proj.Q_mlp2.weights.set(block.mlp.W_mlp2.weights.get())
            block_proj.Q_mlp2.biases.set(block.mlp.W_mlp2.biases.get())
            
            ## pin/tie feedback synapses to transpose of forward ones

            block.attention.E_attn.weights.set(jnp.transpose(block.attention.W_attn_out.weights.get()))
            block.mlp.E_mlp.weights.set(jnp.transpose(block.mlp.W_mlp2.weights.get()))  
            block.mlp.E_mlp1.weights.set(jnp.transpose(block.mlp.W_mlp1.weights.get()))
  
        self.projection.Q_out.weights.set(self.output.W_out.weights.get())
        self.projection.Q_out.biases.set(self.output.W_out.biases.get())
        self.projection.q_target_Ratecell.j_td.set(jnp.zeros((config.batch_size * config.seq_len, config.vocab_size)))
        
        ## pin/tie feedback synapses to transpose of forward ones
       
        self.output.E_out.weights.set(jnp.transpose(self.output.W_out.weights.get()))
        
        ## Perform P-step (projection step)
        self.clamp_input(obs)
        self.clamp_infer_target(lab)
        self.project.run(t=0., dt=1.)
        # initialize dynamics of generative model latents to projected states for the errors it's 0
        self.blocks[0].attention.z_qkv.z.set(self.projection.blocks[0].q_qkv_Ratecell.z.get())
        self.blocks[0].mlp.z_mlp.z.set(self.projection.blocks[0].q_mlp_Ratecell.z.get())
        self.blocks[0].mlp.z_mlp2.z.set(self.projection.blocks[0].q_mlp2_Ratecell.z.get())
        self.output.z_out.z.set(self.projection.q_out_Ratecell.z.get())
        self.output.e_out.dmu.set(self.projection.eq_target.dmu.get())
        self.output.e_out.dtarget.set(self.projection.eq_target.dtarget.get())
        
        
        ## get projected prediction (from the P-step)
        y_mu_inf = self.projection.q_target_Ratecell.z.get()
    
        EFE = 0. ## expected free energy
        y_mu = 0.
        if adapt_synapses:
            for ts in range(0, self.T):
                # self.circuit.clamp_input(obs) ## clamp input data to z_embed & q_embed input compartments
                # self.circuit.clamp_target(lab) ## clamp target data to z_target
                self.clamp_input(obs)
                self.clamp_target(lab)
                # self.circuit.advance(t=ts, dt=1.)
                self.advance.run(t=ts,dt=1.)
           
        y_mu = self.output.W_out.outputs.get() ## get settled prediction

        L1 = self.embedding.e_embed.L.get()
        L4 = self.output.e_out.L.get()
            # Sum errors from ALL blocks
        block_errors = 0.
        for i in range(self.n_layers):
                block = self.blocks[i]
                block_errors += block.attention.e_attn.L.get() + block.mlp.e_mlp.L.get() + block.mlp.e_mlp1.L.get()

        EFE = L4 + block_errors + L1

        if adapt_synapses == True:
                self.embedding_evolve.run()
                self.evolve.run(t=self.T,dt=1.)
                
        ## skip E/M steps if just doing test-time inference
        return y_mu_inf, y_mu, EFE

    def get_latents(self):
        return self.q_out_Ratecell.z.get()
    
    